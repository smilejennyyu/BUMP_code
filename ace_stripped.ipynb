{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import data_load\n",
    "import pandas as pd\n",
    "from src.s3_utils import pandas_from_csv_s3\n",
    "import re"
   ]
  },
  {
   "source": [
    "# Data processing: Join PHQ9, GAD7 and ACE datasets together by record_id and redcap_event_name"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_load(data_keys={'phq9', 'generalized_anxiety_disorder_scale_gad7', 'ace', 'surveys', 'study_ids', 'check_in_adherence_log'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcomes = pd.merge(data['phq9'], data['generalized_anxiety_disorder_scale_gad7'],  how='outer', left_on=['record_id','redcap_event_name'], right_on = ['record_id','redcap_event_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.merge(data['ace'].drop(columns=['redcap_event_name']).dropna(), outcomes, how='left', on='record_id')"
   ]
  },
  {
   "source": [
    "# Convert redcap_event_name to date for PHQ9, GAD7 and ACE datasets"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read study ids\n",
    "id_df = data['study_ids'][['record_id', 'evidation_id']]\n",
    "id_df.rename(columns={'evidation_id': 'user_id'}, inplace=True)\n",
    "\n",
    "# add ids to survey\n",
    "overall_df = overall_df.merge(id_df, on=['record_id'])\n",
    "overall_df.user_id = overall_df.user_id.fillna(-1).astype(int)\n",
    "\n",
    "# standarize naming convention for easier processing later on\n",
    "overall_df.redcap_event_name = overall_df.redcap_event_name.replace('postnatal_checkin_arm_1','postnatal_ci_1_arm_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read check-in dates\n",
    "ci_df = data['check_in_adherence_log']\n",
    "cols = ['record_id'] + [col for col in ci_df.columns if '_date' in col]\n",
    "ci_df = ci_df[cols]\n",
    "\n",
    "# standarize naming convention for easier processing later on\n",
    "ci_df = ci_df.rename(columns={'checkin_postnatal_date': 'checkin_postnatal_date_1'})\n",
    "\n",
    "# add dates to survey, need to map it using the check_in_adherence_log\n",
    "def conver_checkin_string(x):\n",
    "    x = x.split('_arm')[0] #delete all characters after the word 'arm'\n",
    "    num = int(re.search(r'\\d+', x).group())\n",
    "    if 'postnatal' in x:\n",
    "        return f'checkin_postnatal_date_{num}'\n",
    "    else:\n",
    "        return f'checkin_{num}_date'\n",
    "\n",
    "# map checkin_postnature_date_{num} OR checkin_{num}_date to the actual date\n",
    "def map_date(x):\n",
    "    checkin_string_col = x['checkin_string']\n",
    "    return x[checkin_string_col]\n",
    "\n",
    "overall_df = overall_df.merge(ci_df, on=['record_id'])\n",
    "overall_df['checkin_string'] = overall_df.redcap_event_name.apply(conver_checkin_string)\n",
    "overall_df['date'] = overall_df.apply(map_date, axis=1)\n",
    "overall_df = overall_df[overall_df.columns.drop(list(overall_df.filter(regex='checkin_')))]\n",
    "overall_df['date'] = pd.to_datetime(overall_df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 10000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('max_colwidth', None)\n",
    "overall_df.loc[overall_df['record_id'] == 28][['date', 'redcap_event_name']] #check if dates are correctly processed"
   ]
  },
  {
   "source": [
    "# Add Global survey data - PROMIS quality of life"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promis_survey = data['surveys']\n",
    "promis_survey = promis_survey.loc[promis_survey['question_id'] == 121]\n",
    "promis_survey['date'] = pd.to_datetime(promis_survey['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promis_survey['question_text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promis_survey['answer_text'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "promis_survey.loc[promis_survey['user_id'] == 28]"
   ]
  },
  {
   "source": [
    "# Process PHQ9, GAD and PROMIS data by taking the average over time for each individual"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # more balanced set up\n",
    "# ace_levels = {\n",
    "#     (0, 1) : 0,\n",
    "#     (2, 10): 1\n",
    "# }\n",
    "# phq9_levels = {\n",
    "#     (0, 4) : 0,\n",
    "#     (5, 9): 1,\n",
    "#     (10, 14): 1,\n",
    "#     (15, 19): 1,\n",
    "#     (20, 27): 1\n",
    "# }\n",
    "# gad_levels = {\n",
    "#     (0, 4) : 0,\n",
    "#     (5, 9): 1,\n",
    "#     (10, 14): 1,\n",
    "#     (15, 21): 1\n",
    "# }\n",
    "# promis_levels = {\n",
    "#     \"Always\" : 1,\n",
    "#     \"Often\": 1,\n",
    "#     \"Sometimes\": 1,\n",
    "#     \"Rarely\": 0,\n",
    "#     \"Never\": 0\n",
    "# }\n",
    "\n",
    "# from literature\n",
    "ace_levels = {\n",
    "    (0, 4) : 0,\n",
    "    (5, 10): 1\n",
    "}\n",
    "phq9_levels = {\n",
    "    (0, 4) : 0,\n",
    "    (5, 9): 1,\n",
    "    (10, 14): 2,\n",
    "    (15, 19): 3,\n",
    "    (20, 27): 4\n",
    "}\n",
    "gad_levels = {\n",
    "    (0, 4) : 0,\n",
    "    (5, 9): 1,\n",
    "    (10, 14): 2,\n",
    "    (15, 21): 3\n",
    "}\n",
    "promis_levels = {\n",
    "    \"Always\" : 4,\n",
    "    \"Often\": 3,\n",
    "    \"Sometimes\": 2,\n",
    "    \"Rarely\": 1,\n",
    "    \"Never\": 0\n",
    "}\n",
    "def map_levels(x, map_dict):\n",
    "    for key in map_dict:\n",
    "        if isinstance(x, str):\n",
    "            if x == key:\n",
    "                return map_dict[key]\n",
    "        else:\n",
    "            if x >= key[0] and x <= key[1]:\n",
    "                return map_dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df = pd.DataFrame(columns=['user_id', 'ace_sum', 'phq9_sum', 'gad_sum', 'promis_mean'])\n",
    "for uid in overall_df['user_id'].unique():\n",
    "    each_df = overall_df.loc[overall_df['user_id']==uid]\n",
    "    ace_lst = [f'ace_{x}' for x in range(1,11)]\n",
    "    phq9_lst = [f'phq9_{x}' for x in range(1,11)]\n",
    "    gad_lst = [f'gad_{x}' for x in range(1,9)]\n",
    "    ace_sum = each_df[ace_lst].sum(axis=1)\n",
    "    ace_sum_mean = ace_sum.apply(map_levels, map_dict=ace_levels).mean()\n",
    "    phq9_sum = each_df[phq9_lst].sum(axis=1)\n",
    "    phq9_sum_mean = phq9_sum.apply(map_levels, map_dict=phq9_levels).mean()\n",
    "    gad_sum = each_df[gad_lst].sum(axis=1)\n",
    "    gad_sum_mean = gad_sum.apply(map_levels, map_dict=gad_levels).mean()\n",
    "    each_promis_df = promis_survey.loc[promis_survey['user_id']==uid]['answer_text'].apply(map_levels, map_dict=promis_levels)\n",
    "    promis_mean = each_promis_df.mean()\n",
    "    processed_overall_df = processed_overall_df.append({'user_id': uid, 'ace_sum': ace_sum_mean, 'promis_mean': promis_mean, 'phq9_sum': phq9_sum_mean, 'gad_sum': gad_sum_mean}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df = processed_overall_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_overall_df.to_csv('processed_causal_ace_4nodes_ref_levels_reverse.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processed_overall_df[['phq9_sum', 'gad_sum', 'promis_mean']] = processed_overall_df[['phq9_sum', 'gad_sum', 'promis_mean']] * 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df['ace_sum'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df['phq9_sum'].round().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df['gad_sum'].round().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df['promis_mean'].round().hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_overall_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx\n",
    "import notears.notears as notears\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = processed_overall_df[['ace_sum', 'phq9_sum', 'gad_sum', 'promis_mean']].to_numpy().tolist()\n",
    "output_dict = notears.run(notears.notears_standard, data, notears.loss.least_squares_loss, notears.loss.least_squares_loss_grad, e=1e-8, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Acyclicity loss: {}'.format(output_dict['h']))\n",
    "print('Least squares loss: {}'.format(output_dict['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(output_dict['W'])\n",
    "plt.title(\"Learned adjacency matrix\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acyclic_W = notears.utils.threshold_output(output_dict['W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(acyclic_W)\n",
    "plt.title(\"Learned adjacency matrix (thresholded)\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = networkx.DiGraph(acyclic_W)\n",
    "networkx.draw(G, with_labels=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_G = networkx.DiGraph((output_dict['W'] * acyclic_W).round(1))\n",
    "layout = networkx.spring_layout(weighted_G)\n",
    "networkx.draw(weighted_G, layout, node_size=1000, with_labels=True, font_weight='bold',    font_size=15)\n",
    "labels = networkx.get_edge_attributes(weighted_G,'weight')\n",
    "networkx.draw_networkx_edge_labels(weighted_G,pos=layout,edge_labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
